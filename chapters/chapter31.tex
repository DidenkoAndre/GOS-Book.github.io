\chapter{Математическое ожидание и дисперсия случайной величины, их свойства.}
\section{Случайные величины}

Пусть дано конечное вероятностное пространство $(\Omega,\mathcal A, P)$, где $P$ --- вероятности каждого события, состоящего из элементарных исходов из $\Omega$. Тогда \textit{случайной величиной} принято называть любую функцию $\xi\colon \Omega \to \bbR$ (т.е. случайному элементарному исходу ставится в соответствие совершенно конкретное значение).

Абсолютно так же определяется случайная величина для бесконечного счетного вероятностого пространства, где $\Omega = {w_1,\dots, w_n,...}$.

Пусть теперь дано произвольное вероятностное пространство $(\Omega,\mathcal A, P)$, где снова $P$ --- вероятности каждого события, состоящих из элементарных исходов из $\Omega$ (вероятностная мера, строго говоря). Тогда в этом общем случае:
\begin{defn}
\textit{Случайной величиной $\xi$} называется действительная функция от элементарного события $w$: $\xi = \xi(w), \; w\in\Omega$, для которой при любом действительном $x$ множество $\{w: \xi(w) \le x\}$ принадлежит $\mathcal A$ (т.~е. является событием) и для него определена вероятность $P(w: \xi(w) \le x)$, записываемая кратко $F_\xi(x)=P(\xi \le x)$. Эта вероятность, рассматриваемая как функция $x$, называется \textit{функцией распределения случайной величины~$\xi$}.
\end{defn}
Отметим ее свойства:
\begin{enumerate}
\item 
$0 \le F(x) \le 1 \fa x \in \bboR;$ 
\item
$F(x_1) \le F(x_2), \text{если}\; x_1\le x_2;$
\item
$F(-\infty)=0, \; F(+\infty)=1;$
\item
$F(x+0)=F(x)$ --- непрерывна справа.
\end{enumerate}

Важным классом распределений вероятностей являются \textit{абсолютно непрерывные распределения}, для которых существует неотрицательная функция $p(z)$, удовлетворяющая при любых $x$ равенству:
$$F_\xi(x)=\int_{-\infty}^{x}p(z)\,dz,$$ где $p(z)$ называют \textit{плотностью вероятности}, обладающая следующими свойствами:
\begin{enumerate}
\item 
$p(x)\ge 0, \fa x \in \bbR;$ 
\item 
$\forall x_1,x_2: P(x_1\le\xi<x_2)=\int_{x_1}^{x_2}p(x)\,dx$
\item
$\int_{-\infty}^{+\infty}p(x)\,dx=1$
\end{enumerate}

Другой класс составляют \textit{дискретные распределения}, задаваемые конечным или счетным набором вероятностей $P(\xi = x_k)$ для которых
$$
\sum\limits_k P(\xi=x_k)=1,
$$
тогда функция распределения $F_\xi(x)=\sum\limits_{k:\; x_k \le x} P(\xi=x_k)$.

Если распределение случайной величины абсолютно непрерывно или дискретно, то говорят также, что сама случайная величина или ее функция распределения соответственно абсолютно непрерывны или дискретны.

Нужно подчеркнуть, что распределения не делятся лишь только на дискретные и непрерывные. Возможны случаи, когда случайные величины не являются ни дискретными, ни непрерывными (например, взять хотя бы сумму непрерывной и дискретной случайной величины). Но в рамках программы ГОСа мы рассмотрим только непрерывные и дискретные случайные величины. 

\section{Совместные распределения нескольких случайных величин}
Пусть на вероятностном пространстве заданы случайные величины: $\xi_1,\xi_2,\dots,\xi_n$.
\begin{defn}
\textit{Совместной функцией распределения} (или\textit{ многомерной функцией распределения}) величин $\xi_1,\xi_2,\dots,\xi_n$ (или случайного вектора $\xi = (\xi_1,\xi_2,\dots,\xi_n)$) называется вероятность 
$$
F_\xi(x)=F_{\xi_1,\dots,\xi_n}(x_1,\dots,x_n)=P(\xi_1 < x_1,\dots,\xi_n<x_n).
$$
\end{defn}

Когда n-мерный вектор $(\xi_1,\xi_2,\dots,\xi_n)$ имеет плотностью распределения вероятностей $p(x_1,x_2,\dots,x_n)$ (называемой \textit{совместной плотностью распределения}), то функцию распределения можно записать в виде интеграла:
$$
F(x_1,\dots,x_n)=\idotsint\limits_D p(z_1,\dots,z_n)\,dz_1\,dz_2\dots\,dz_n,
$$
причем область интегрирования $D$ определяется неравенствами $\xi_i<x_i, i\in \overline{1,n}$

Очевидно, что в случае дискретных случайных величин совместную функцию распределения можно записать как n-мерную сумму, также распространенную на область $D$.


Решим задачу о функции распределения суммы непрерывных случайных величин $\zeta=\xi+\eta$. Пусть $p(x_1,x_2)$ --- плотность распределения вероятностей вектора $(\xi,\eta)$. Искомая функция равна вероятности попадания точки $(\xi,\eta)$ в полупространство $\xi+\eta<x$ и, следовательно, 
\begin{multline}\label{ch31.1eq1}
F_{\zeta}(x)=\iint\limits_{x_1+x_2<x} p(x_1,x_2)\,dx_1\,dx_2=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{x-x_1}p(x_1,x_2)\,dx_1\,dx_2=\\
=\int\limits_{-\infty}^{x}dx_1\int\limits_{-\infty}^{+\infty} p(z,x_1-z)\,dz.
\end{multline}

Сформулируем напоследок одно важное определение.
\begin{defn} Случайные величины $\xi_1,\dots,\xi_n$ называются \textit{независимыми}, если для любых числовых множеств $B_1,\dots, B_k\ \forall k\in\overline{1,n}$, для которых определены вероятности событий $\{\xi_j\in B_j\}$ имеет место равенство
$$
P(\xi_1\in B_1,\dots, \xi_k \in B_k)=P(\xi_1\in B_1)\cdot\dots\cdot P(\xi_k \in B_k).
$$
\end{defn}

Если положить $B_k=(-\infty;x_k)$, то из только что озвученного определения можно вывести, что для независимых величин верно следующее свойство:
$$
F_{\xi_1,\dots\xi_n}(x_1,\dots,x_n)=F_{\xi_1}(x_1)\dots F_{\xi_n}(x_n).
$$

Отметим, что две дикретные случайные величины $\xi$ и $\eta$ со значениями $x_1,\dots,x_n$ и $y_1,\dots,y_n$ соответсвенно будут независимы тогда и только тогда, когда для любых $i,j$:
$$
P(\xi = x_i, \eta = y_j) = P (\xi = x_i)\cdot P(\eta=y_j).
$$

Также отметим, что две непрерывные случайные величины $\xi$ и $\eta$ будут независимы тогда и только тогда, когда во всех точках непрерывности функций $p_\xi(x),p_\eta(y),p_{\xi\eta}(x,y)$ --- плотностей распределений:
$$
p_{\xi\eta}(x,y)=p_\xi(x)\cdot p_\eta(y).
$$ 

\section{Математическое ожидание}

Сначала дадим определение для дискретных случайных величин. Пусть $x_1,x_2,\dots,x_n,\dots$ обозначают возможные значения случайной величины $\xi$, а $p_1,p_2,\dots,p_n,\dots$ --- соответствующие им вероятности, $\Omega = \{w_1,\dots, w_n,...\}$~--- наше пространство элементарных исходов в $(\Omega,\mathcal A, P)$.
\begin{defn}\label{teorver1}
\textit{Математическим ожиданием дискретной случайной величины} $\xi$ называется число, обозначаемое $M\xi$ и равное
\begin{equation}
M\xi = \sum_{w \in \Omega} \xi(w)\cdot P(w),
\end{equation}
где $P(w)$ --- элементарные вероятности, если этот ряд сходится абсолютно.
\end{defn}
Перепишем определение матожидания по-другому.
\begin{multline*}
M\xi=\sum_{w \in \Omega} \xi(w)\cdot P(w)=x_1\left(\textstyle\sum\limits_{w\colon \xi(w)=x_1} P(w)\right)+\dots+x_k\left(\textstyle\sum\limits_{w\colon \xi(w)=x_k} P(w)\right)+\\+\dots =x_1P(\xi=x_1)+\dots+ x_kP(\xi=x_k)+\dots=\sum_{k\in \bbN} x_kP(\xi=x_k)
\end{multline*}
В силу этого равентсва, дадим аналогичное определение матожиданию:
\begin{defnn}\label{teorver1s}
\textit{Математическим ожиданием дискретной случайной величины} $\xi$ называется сумма ряда $\sum\limits_{k=1}^{\infty}x_k p_k$, если тот сходится абсолютно.
\end{defnn}


Для непрерывных случайных величин естественным будет следующее определение: 
\begin{defn} 
Если случайная величина $\xi$ непрерывна, $p(x)$ --- ее плотность распределения и $F_\xi(x)$ --- функция распределения,  то \textit{математическим ожиданием} \textit{случайной величины} $\xi$ называется интеграл
\begin{equation}
M\xi\triangleq\int_{-\infty}^{+\infty} xp(x)\,dx = \int_{-\infty}^{+\infty} x\,dF_\xi(x),
\end{equation}
в тех случаях, когда существует интеграл $\int_{-\infty}^{+\infty} |x|p(x)\,dx.$ 
\end{defn}



\section{Теоремы о математическом ожидании}
\begin{thm}\label{ch31.3thm1}
Математическое ожидание постоянной равно этой постоянной.
\end{thm}
\begin{proof}
Постоянную $C$ мы можем рассматривать, как дискретную случайную величину, которая может принимать только одно значение $C$ с вероятностью единица; поэтому
\begin{equation*}
M C = C\cdot1=C, \qedhere
\end{equation*}
\end{proof}
\begin{thm}\label{ch31.2t1}
Для любых случайных величин $\xi$ и $\eta$, для которых существуют математические ожидания $M\xi$ и $M\eta$ справедливо\footnote{Условие на существование математических ожиданий очень важно. Если оно не выполнено, то математическое ожидание суммы не обязано быть равно сумме матожиданий, подобно тому как и сумма двух несходящихся интегралов может неожиданно сойтись.}
\begin{equation}
M(\xi+\eta) = M\xi+M\eta.
\end{equation}
\end{thm}
\begin{proof}
Рассмотрим сначала случай дискретных случайных величин $\xi$ и $\eta$. Пусть $a_1,\dots,a_n,\dots$ --- возможные значения величины $\xi$ и $p_1,\dots,p_n,\dots$ --- их вероятности; $b_1,\dots,b_k,\dots$ --- возможные значения величины $\eta$ и $q_1,\dots,q_k,\dots$ --- вероятности этих значений. Возможные значения величины $\xi+\eta$ имеют вид $a_n+b_k\ (k,n\in\bbN).$ Обозначим через $p_{nk}$ вероятность того, что $\xi$ примет значение $a_n$, а $\eta$ --- значение $b_k$. По определению математического ожидания
\begin{multline*}
M(\xi+\eta)=\sum\limits_{n,k=1}^{\infty} (a_n+b_k)p_{nk}=\sum\limits_{n=1}^{\infty}\sum\limits_{k=1}^{\infty} (a_n+b_k)p_{nk}=\\
=\sum\limits_{n=1}^{\infty} a_n \left(\sum\limits_{k=1}^{\infty}p_{nk}\right)+\sum\limits_{k=1}^{\infty} b_k \left(\sum\limits_{n=1}^{\infty}p_{nk}\right).
\end{multline*}
Так как по теореме о полной вероятности $\sum\limits_{k=1}^{\infty}p_{nk}=p_n$ и $\sum\limits_{n=1}^{\infty}p_{nk}=q_k$, то 
$$
\sum\limits_{n=1}^{\infty} a_n \left(\sum\limits_{k=1}^{\infty}p_{nk}\right) = \sum\limits_{n=1}^{\infty} a_n p_n = M\xi\ \text{и}\ \sum\limits_{k=1}^{\infty} b_k \left(\sum\limits_{n=1}^{\infty}p_{nk}\right)=\sum\limits_{k=1}^{\infty} b_k q_k = M\eta.
$$
Доказательство теоремы для случая дискретных слагаемых завершено.

Точно так же в случае, когда существуют двумерная плотность распределения $p(x,y)$ случайной величины $(\xi,\eta)$, по формуле \eqref{ch31.1eq1} находим:
\begin{multline*}
M(\xi+\eta)=\int x\,dF_{\xi+\eta}(x)=\int x \left(\int p(z,x-z)\,dz\right)\,dx=\iint xp(z,x-z)\,dz\,dx=\\=\iint (z+y) p(z,y)\,dz\,dy=\iint z p(z,y)\,dz\,dy + \iint y p(z,y)\,dz\,dy=\\=\int z p_\xi(z)\,dz + \int y p_\eta (y)\,dy= M\xi+M\eta.
\end{multline*}

Эта теорема имеет место и в самом общем случае, но мы не будем его касаться в данном пособии.\footnote{Мы не будем их затрагивать, потому что их просто не требуют по программе ГОСа, да и в нашем курсе теории вероятностей не давалось общего случая.}
\end{proof}

\begin{thm}[мультипликативность]
Математическое ожидание произведения независимых случайных величин $\xi$ и $\eta$, для которых существуют математические ожидания $M\xi$ и $M\eta$ равно произведению их математических ожиданий.
$$
M (\xi\eta)=M\xi\cdot M\eta.
$$
\end{thm}
\begin{proof}
Рассмотрим сначала случай дискретных случайных величин $\xi$ и $\eta$. Пусть $a_1,\dots,a_n,\dots$ --- возможные значения величины $\xi$ и $p_1,\dots,p_n,\dots$ --- их вероятности; $b_1,\dots,b_k,\dots$ --- возможные значения величины $\eta$ и $q_1,\dots,q_k,\dots$ --- вероятности этих значений. Тогда вероятность того, что $\xi$ примет значение $a_n$, а $\eta$ --- значение $b_k$ равна $p_n q_k$. По определению математического ожидания 
$$
M(\xi\eta)=\sum\limits_{k,n}a_n b_k p_n q_k = \sum\limits_{n=1}^{+\infty}\sum\limits_{k=1}^{+\infty} a_n b_k p_n q_k =\left(\sum\limits_{n=1}^{+\infty}a_n p_n\right)\left(\sum\limits_{k=1}^{+\infty}b_k q_k\right)=M\xi\cdot M\eta.
$$

Аналогично, если $\xi$ и $\eta$ --- абсолютно непрерывные случайные величины, и $p_{\xi\eta}(x,y)$ --- их плотность распределения. Так как они независимы, то
$$
p_{\xi\eta}(x,y)=p_\xi(x)p_\eta(y).
$$
Тогда по формуле математического ожидания для непрерывного случая:
\begin{multline*}
M(\xi\eta)=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}xyp_{\xi\eta}(x,y)\,dx\,dy=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}xyp_\xi(x)p_\eta(y)(x,y)\,dx\,dy=\\=\int\limits_{-\infty}^{+\infty}xp_\xi(x)\,dx\int\limits_{-\infty}^{+\infty}yp_\eta(y)\,dy=M\xi\cdot M\eta.
\end{multline*}

Эта теорема имеет место и в самом общем случае, но мы не будем его касаться в данном пособии.
\end{proof}
\begin{cons}
Математическое ожидание произведения независимых случайных величин $\xi_1,\xi_2,\dots, \xi_n$, для которых существуют математические ожидания $M\xi_1,M\xi_2,\dots, M\xi_n$ равно произведению их математических ожиданий.
$$
M (\xi_1\dots\xi_n)=M\xi_1\cdot\dots\cdot M\xi_n.
$$
\end{cons}

\begin{cons}\label{ch31.2c2}
Постоянный множитель можно выносить за знак математического ожидания
$$
M(C\xi)=C\cdot M\xi
$$
\end{cons}
\begin{proof}
Постоянную $C$ и случайную величину $\xi$ (какой бы она ни была) можно рассматривать как независимые величины.\footnote{Точнее и проще говоря, константа не зависит от элементарных исходов $\Omega$, по которым берется сумма/интеграл в выражении для матожидания, поэтому эту константу можно просто вытащить за знак суммы/интеграла.}
\end{proof}
\begin{cons}(линейность) Для любых случайных величин $\xi_1,\dots,\xi_n$, таких, что существуют математические ожидания $M\xi_1,\dots, M\xi_n$ и любых чисел $c_1,\dots, c_n$ справедливо
$$
M(c_1\xi_1+c_2\xi_2+\dots+c_n\xi_n) = c_1M\xi_1+c_2M\xi_2+\dots+c_nM\xi_n
$$
\end{cons}
\begin{proof} Это утверждение доказывается по индукции с помощью теоремы \ref{ch31.2t1} и следствия \ref{ch31.2c2}.
\end{proof}
\begin{thm}[монотонность] \label{ch31.2t4}
Если случайные величины $\xi$ и $\eta$ таковы, что $\xi \ge\eta$ и существуют математические ожидания $M\xi$ и $M\eta$, то верно
$$
M\xi\ge M\eta.
$$
\end{thm}
\begin{proof}
Докажем сначала, что из $\xi\ge 0$ следует $M\xi\ge0$. В самом деле, в случае дискретных случайных величин в определении математического ожидания для все слагаемые неотрицательны (значения $\xi \ge 0$ и вероятности больше нуля). Аналогично, в случае непрерывных случайных величин в определении математического ожидания подынтегральное выражение неотрицательно (значения $\xi \ge 0$ и плотность больше нуля). Тогда в силу свойства монотонности соответсвующих сумм и интегралов получаем, что математическое ожидание $M\xi \ge 0$. Применяя доказанное свойство к неотрицательной разности $\xi-\eta \ge 0$, получаем $M(\xi-\eta)=M\xi-M\eta\ge0$, что и требовалось доказать.

Эта теорема имеет место и в самом общем случае, но мы не будем его касаться в данном пособии.
\end{proof}
\begin{thm}\label{ch31.2t3}
Если случайная величина $\xi$ такова, что $\xi\ge 0$ и $M\xi=0$, то $\xi=0$.
\end{thm}
\begin{proof}
Для дискретных случайных величин из $M\xi=0$ следует, что $\sum\limits_{n=1}^{\infty} x_n p_n = 0$, где $p_n > 0 \ \forall n\in \bbN$ и по условию $x_n \ge 0 \ \forall n\in \bbN$. А значит такое возможно только при $x_n = 0 \; \forall n\in \bbN$.

Аналогично доказывается для непрерывных случайных величин, и эта теорема имеет место быть в самом общем случае.
\end{proof}

\section{Дисперсия}
\begin{defn}
\textit{Дисперсией случайной величины $\xi$} называется математическое ожидание квадрата отклонения $\xi$ от $M\xi$, если $M(\xi-M\xi)^2$ существует. Обозначим ее $D\xi$. 
\begin{equation} \label{ch31.4eq1}
D\xi=M(\xi-M\xi)^2.
\end{equation}
\end{defn}

Дисперсия играет роль меры рассеяния (разбросанности) значений случайной величины около математического ожидания.

Заметим, что в силу линейности математического ожидания и теоремы~\ref{ch31.3thm1}\; и т.к. математическое ожидание по сути это постоянное число (точнее предел), то $M(M\xi)=M\xi$:
\begin{multline*}
D\xi = M(\xi-M\xi)^2=M(\xi^2-2\xi M\xi+(M\xi)^2)=M\xi^2-2M\xi M\xi+(M\xi)^2=\\=M\xi^2-(M\xi)^2
\end{multline*}
Так как дисперсия является неотрицательной величиной (покажем ниже), то из последнего мы выводим одно свойство матожидания: $M\xi^2\ge(M\xi)^2$.

Отметим основные свойства дисперсии.

\begin{thm} Дисперсия любой случайной величины неотрицательна, причем $D\xi = 0$ тогда и только тогда, когда $\xi$ --- постоянная. 
\end{thm}
\begin{proof}
Свойство неотрицательности следует из неравенства $(\xi - M \xi)^2 \ge 0$ и свойства монотонности математического ожидания: $D \xi = M (\xi - M \xi)^2 \ge 0$.

Если $\xi = c$, $c$ --- постоянная, то $D c = M (c - M c)^2 = 0$. 

Если $D\xi=M (\xi - M \xi)^2=0$, то учитывая, что $(\xi - M \xi)^2\ge 0$ из теоремы \ref{ch31.2t3} получаем, что $(\xi - M \xi)^2=0$, т.е. $\xi=M\xi$, а так как математическое ожидание~--- постоянное число, то мы доказали нашу теорему в обе стороны.
\end{proof}
\begin{thm} Если $a$ --- постоянная, то $$D(a\xi) = a^2 D\xi.$$
\end{thm}
\begin{proof}
Действительно, $D(a\xi) = M(a\xi - M (a\xi))^2 = M [a (\xi - M \xi)]^2 = a^2M(\xi - M\xi)^2 = a^2 D\xi.$
\end{proof}

\begin{thm}\label{ch31.disp1}
Если случайные величины $\xi$ и $\eta$ независимы, то $$D(\xi + \eta) = D\xi + D\eta.$$
\end{thm}
\begin{proof}
Используя определение дисперсии $\eqref{ch31.4eq1}$ и свойство линейности математического ожидания, получим
$$
D(\xi + \eta) = M[(\xi + \eta) - M(\xi + \eta)]^2 = M(\xi - M\xi)^2 + 2M(\xi - M\xi)(\eta - M\eta) + M(\eta  - M\eta)^2.
$$

Отсюда следует формула из теоремы \ref{ch31.disp1}, так как согласно свойству мультипликативности математического ожидания
\begin{multline*}
M(\xi - M\xi)(\eta - M\eta) = M(\xi - M\xi)M(\eta - M\eta) =\\= (M\xi - M\xi)(M\eta - M\eta) = 0.\tag*{\qedhere}
\end{multline*}
\end{proof}

Формула из теоремы \ref{ch31.disp1} по индукции распространяется на сумму $n$ попарно независимых случайных величин. 
\begin{cons}
Если $\xi_1,\xi_2, \ldots, \xi_n$ попарно независимы, то 
$$
D(\xi_1 + \xi_2 + \ldots + \xi_n) = D(\xi_1) + D(\xi_2) + \ldots + D(\xi_n).
$$
\end{cons}

\section{Ковариация}
\begin{defn}
Величина $M[(\xi - M\xi)(\eta - M\eta)]$ носит название \textit{ковариации} между $\xi$ и $\eta$ и обозначается $$\cov (\xi,\eta) = M[(\xi - M\xi)(\eta - M\eta)].$$
\end{defn}

Теперь можно обощить теорему \ref{ch31.disp1} на случай зависимых случайных величин.
\begin{thm}
Имеет место формула
$$
D(\xi_1 + \xi_2 + \ldots + \xi_n) = \sum\limits_{k=1}^{n} D(\xi_k) + 2 \sum\limits_{1\le k < l \le n} \cov (\xi_k,\xi_l).
$$
\end{thm}
\begin{proof}
Доказательство для двух случайных величин ничем не отличается от данного в теореме \ref{ch31.disp1}, кроме того, что замечаем  $M(\xi - M\xi)(\eta - M\eta)=\cov (\xi,\eta).$ По индукции верно и для произвольного натурального числа $n$.
\end{proof}
\begin{thm}[Неравенства Коши-Буняковского]
Для любых двух случайных величин $\xi, \eta$
\begin{align}
&|M(\xi\eta)| \le \sqrt{M\xi^2 \cdot M\eta^2}.\\
&|\cov(\xi,\eta)| \le \sqrt{D\xi\cdot D\eta}.\label{chHZeq17}
\end{align}
\end{thm}
\begin{proof}
Для любых чисел $x, y$ по теореме \ref{ch31.2t4} о математическом ожидании 
$$
M(x\xi + y\eta)^2 \ge 0.
$$

Отсюда следует, что квадратичная формула
$$
x^2M\xi^2 + 2xyM\xi\eta + y^2M\eta^2
$$
неотрицательно определена, а следовательно, ее дискриминант неположителен:
$$
(M(\xi\eta))^2 - M\xi^2 \cdot M\eta^2 \le 0.
$$

Аналогично можно доказать второе неравенство \eqref{chHZeq17} Коши-Буняковского, взяв в качестве неотрицательной функции $D(x\xi + y\eta)\ge 0$.\footnote{Кстати второе неравенство КБ более естественное, потому что именно ковариация играет роль скалярного произведения в пространстве случайных величин с конечным вторым моментом. Неравенство КБ --- неотъемлемое свойство именно скалярного произведения.}
\end{proof}

\begin{defn}
Величина $\displaystyle\frac{\cov(\xi,\eta)}{\sqrt{D\xi\cdot D\eta}}$ называется коэффициентом коррелляции между $\xi$ и $\eta$  и обозначается $$\rho (\xi,\eta) = \frac{\cov(\xi,\eta)}{\sqrt{D\xi\cdot D\eta}}.$$
\end{defn}

Обсудим пару свойств коэффициента корреляции.
\begin{itemize}
\item 
$|\rho(\xi,\eta)|\le1$.

Это следует из неравенства Коши-Буняковского $|\cov(\xi,\eta)| \le \sqrt{D\xi\cdot D\eta}$.
\item
Если $\xi$, $\eta$ независимы, то $\rho(\xi,\eta)=0$.

Это было попутно получено в теореме \ref{ch31.disp1}. Повторим еще раз. В случае независимых $\xi$ и $\eta$.
\begin{multline*}
\cov(\xi,\eta)=M(\xi - M\xi)(\eta - M\eta) = M(\xi - M\xi)M(\eta - M\eta)=\\ = (M\xi - M\xi)(M\eta - M\eta) = 0.
\end{multline*}
\end{itemize}